{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "17oi1NMojJ_tog5etVQxP0uY4xKvlU1Gk",
      "authorship_tag": "ABX9TyMSAUP+uHemttdKNc8C1gy1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLNinja/HandKeypointsDetection/blob/main/UNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQB_W8MWdl9S"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display Methods for Coordinates and Heatmaps"
      ],
      "metadata": {
        "id": "OjOzKa5UuCfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sample(image, coords, original_size=(224, 224), color='bw'):\n",
        "    # Transform to numpy array\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        image = image.cpu().detach().numpy()\n",
        "    if not isinstance(image, np.ndarray):\n",
        "        original_image = np.array(image)\n",
        "    else:\n",
        "        original_image = image\n",
        "\n",
        "    original_image = original_image.copy()\n",
        "    if original_image.dtype != np.uint8:\n",
        "        original_image = (original_image * 255).astype(np.uint8)\n",
        "\n",
        "    # Scale coordinates to image size\n",
        "    img_h, img_w = original_size\n",
        "    if isinstance(coords, torch.Tensor):\n",
        "        coords = coords.detach().cpu().numpy()\n",
        "    rescaled_keypoints = [(int(x * img_w), int(y * img_h)) for x, y in coords]\n",
        "\n",
        "    # Draw teh lines - relation of keypoints on hand\n",
        "    line_dict = [\n",
        "        [0, 1, 2, 3, 4],\n",
        "        [0, 5, 6, 7, 8],\n",
        "        [9, 10, 11, 12],\n",
        "        [13, 14, 15, 16],\n",
        "        [0, 17, 18, 19, 20],\n",
        "        [5, 9, 13, 17]\n",
        "    ]\n",
        "    for seq in line_dict:\n",
        "        coords = []\n",
        "        for i in seq:\n",
        "            coords.append(rescaled_keypoints[i])\n",
        "        for i in range(len(coords)-1):\n",
        "            cv2.line(original_image, coords[i], coords[i+1], color=(0, 0, 0), thickness=1)\n",
        "\n",
        "    # Draw keypoints on the original image\n",
        "    for (x, y) in rescaled_keypoints:\n",
        "        cv2.circle(original_image, (x, y), radius=2, color=(255, 255, 255), thickness=-1)\n",
        "\n",
        "    # Display the image\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(original_image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Z9GzZRAieAgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displays the original image, combined heatmaps and overlayed heatmaps over the image"
      ],
      "metadata": {
        "id": "7YEsYFSHvLVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_heatmap(image, heatmaps, original_size=(224, 224)):\n",
        "    # Transform to numpy array\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        image = image.squeeze(0)\n",
        "        image = image.permute(1, 2, 0).cpu().numpy()\n",
        "    if not isinstance(image, np.ndarray):\n",
        "        original_image = np.array(image)\n",
        "    else:\n",
        "        original_image = image\n",
        "\n",
        "    original_image = original_image.copy()\n",
        "    if original_image.dtype != np.uint8:\n",
        "        original_image = (original_image * 255).astype(np.uint8)\n",
        "\n",
        "    if isinstance(heatmaps, torch.Tensor):\n",
        "        heatmaps = heatmaps.detach().cpu().numpy()\n",
        "\n",
        "    H, W = image.shape[:2]\n",
        "    combined_heatmap = np.sum(heatmaps, axis=0)\n",
        "    combined_heatmap = np.clip(combined_heatmap, 0, 1)\n",
        "\n",
        "    # Convert to color map for visualization\n",
        "    colored_heatmap = cv2.applyColorMap((combined_heatmap * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
        "    overlayed_image = cv2.addWeighted(image, 0.6, colored_heatmap, 0.4, 0)\n",
        "\n",
        "    # Display\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"Combined Heatmap\")\n",
        "    plt.imshow(combined_heatmap, cmap='hot')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"Overlayed Image\")\n",
        "    plt.imshow(overlayed_image[..., ::-1])  # Convert BGR to RGB\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "YfAAoLJVeCTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate heatmaps based on coordinates"
      ],
      "metadata": {
        "id": "b1xeQreguILI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_heatmap(heatmap_size, keypoints, sigma=2):\n",
        "    num_keypoints = keypoints.shape[0]\n",
        "    heatmaps = np.zeros((num_keypoints, heatmap_size[1], heatmap_size[0]), dtype=np.float32)\n",
        "\n",
        "    for i, (x, y) in enumerate(keypoints):\n",
        "        if x < 0 or y < 0:\n",
        "            continue\n",
        "        x = int(x * heatmap_size[0])\n",
        "        y = int(y * heatmap_size[1])\n",
        "        heatmap = np.zeros((heatmap_size[1], heatmap_size[0]), dtype=np.float32)\n",
        "        heatmap = cv2.circle(heatmap, (x, y), sigma * 3, 1, -1)\n",
        "        heatmap = cv2.GaussianBlur(heatmap, (0, 0), sigma)\n",
        "        heatmap /= heatmap.max() + 1e-5\n",
        "        heatmaps[i] = heatmap\n",
        "    return heatmaps"
      ],
      "metadata": {
        "id": "meIu4c-WeCQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Coordinates from heatmaps"
      ],
      "metadata": {
        "id": "q0-Dbex1ufna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def soft_argmax(heatmaps, beta=100):\n",
        "    B, K, H, W = heatmaps.shape\n",
        "    heatmaps = heatmaps.view(B, K, -1)\n",
        "    heatmaps = F.softmax(heatmaps * beta, dim=2)\n",
        "\n",
        "    indices = torch.arange(H * W, device=heatmaps.device).float()\n",
        "    x_coords = (indices % W).unsqueeze(0)\n",
        "    y_coords = (indices // W).unsqueeze(0)\n",
        "\n",
        "    x = torch.sum(heatmaps * x_coords, dim=2) / W\n",
        "    y = torch.sum(heatmaps * y_coords, dim=2) / H\n",
        "\n",
        "    return torch.stack([x, y], dim=2)\n"
      ],
      "metadata": {
        "id": "6FNRGCDWeCH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Loader"
      ],
      "metadata": {
        "id": "idk9x2JPuNzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class HandKeypointsDataset(Dataset):\n",
        "    def __init__(self, npz_file, img_size=(224, 224)):\n",
        "        self.npz_file = npz_file\n",
        "        self.img_size = img_size\n",
        "\n",
        "        self.data = np.load(self.npz_file)\n",
        "        self.images = self.data['images']\n",
        "        self.keypoints = self.data['keypoints']\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data[\"images\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        keypoints = self.keypoints[idx].reshape(21, 3)\n",
        "        keypoints = keypoints[:, :2]\n",
        "\n",
        "        image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(keypoints, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "2NDrG6nIeCNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition\n",
        "UNET architecture"
      ],
      "metadata": {
        "id": "klW-EOPauQCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=21, init_features=64):\n",
        "        super(UNet, self).__init__()\n",
        "        features = init_features\n",
        "\n",
        "        self.encoder1 = self._block(in_channels, features)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.encoder2 = self._block(features, features * 2)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.encoder3 = self._block(features * 2, features * 4)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.encoder4 = self._block(features * 4, features * 8)\n",
        "        self.pool4 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.bottleneck = self._block(features * 8, features * 16)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
        "        self.decoder4 = self._block(features * 16, features * 8)\n",
        "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
        "        self.decoder3 = self._block(features * 8, features * 4)\n",
        "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
        "        self.decoder2 = self._block(features * 4, features * 2)\n",
        "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
        "        self.decoder1 = self._block(features * 2, features)\n",
        "\n",
        "        self.output = nn.Conv2d(features, out_channels, kernel_size=1)\n",
        "\n",
        "    def _block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(self.pool1(e1))\n",
        "        e3 = self.encoder3(self.pool2(e2))\n",
        "        e4 = self.encoder4(self.pool3(e3))\n",
        "\n",
        "        b = self.bottleneck(self.pool4(e4))\n",
        "\n",
        "        d4 = self.upconv4(b)\n",
        "        d4 = self.decoder4(torch.cat((d4, e4), dim=1))\n",
        "        d3 = self.upconv3(d4)\n",
        "        d3 = self.decoder3(torch.cat((d3, e3), dim=1))\n",
        "        d2 = self.upconv2(d3)\n",
        "        d2 = self.decoder2(torch.cat((d2, e2), dim=1))\n",
        "        d1 = self.upconv1(d2)\n",
        "        d1 = self.decoder1(torch.cat((d1, e1), dim=1))\n",
        "\n",
        "        return self.output(d1)"
      ],
      "metadata": {
        "id": "iD3Al7KveCKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "WYQRZwgyeCD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "sssZNebQeCBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing a model\n",
        "Both models use same architecture and optimizer settings.\n",
        "\n",
        "First model extracts coordinates from predicted heatmaps, computes the loss with ground truth coordinates using L1 Loss.\n",
        "\n",
        "Second model transforms ground truth coordinates it gt heatmaps, computes loss with predicted heatmaps using MSE Loss."
      ],
      "metadata": {
        "id": "bJLDeB9ArHkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'unet_v1'\n",
        "checkpoint = '/content/drive/MyDrive/HandKeypoints/models/handpose_checkpoint_unet.pth'"
      ],
      "metadata": {
        "id": "6qW_Jwi7eB8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'unet_v2'\n",
        "checkpoint = '/content/drive/MyDrive/HandKeypoints/models/handpose_checkpoint_unet_V2.pth'"
      ],
      "metadata": {
        "id": "tTbTGG3mpzQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load pre-trained model from checkpoint\n",
        "if os.path.exists(checkpoint):\n",
        "    checkpoint = torch.load(checkpoint, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    previous_total_loss = checkpoint['total_loss']\n",
        "    previous_avg_loss = checkpoint['avg_loss']\n",
        "    history = checkpoint['history']\n",
        "    best_acc = history['test_accuracy'][-1]\n",
        "    print(f\"Resuming training from epoch {start_epoch}\")\n",
        "else:\n",
        "    start_epoch = 1\n",
        "    previous_acc = 0\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'test_loss': [],\n",
        "        'test_accuracy': [],\n",
        "        'test_avg_time_per_sample': []\n",
        "    }\n",
        "    best_acc = 0.0"
      ],
      "metadata": {
        "id": "Q_trwITbeB5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keypoints_to_heatmaps(keypoints, heatmap_size=(224, 224), sigma=2):\n",
        "    B, K, _ = keypoints.shape\n",
        "    W, H = heatmap_size\n",
        "    device = keypoints.device\n",
        "\n",
        "    # Create a mesh grid for the heatmap\n",
        "    x = torch.arange(W, dtype=torch.float32, device=device)\n",
        "    y = torch.arange(H, dtype=torch.float32, device=device)\n",
        "    yy, xx = torch.meshgrid(y, x, indexing='ij')  # shape: (H, W)\n",
        "\n",
        "    xx = xx[None, None, :, :].expand(B, K, H, W)\n",
        "    yy = yy[None, None, :, :].expand(B, K, H, W)\n",
        "\n",
        "    # Rescale keypoints to pixel coordinates\n",
        "    kp_x = keypoints[:, :, 0] * W  # (B, 21)\n",
        "    kp_y = keypoints[:, :, 1] * H  # (B, 21)\n",
        "    kp_x = kp_x[:, :, None, None]  # (B, 21, 1, 1)\n",
        "    kp_y = kp_y[:, :, None, None]\n",
        "\n",
        "    # Compute squared distances\n",
        "    heatmaps = torch.exp(-((xx - kp_x) ** 2 + (yy - kp_y) ** 2) / (2 * sigma ** 2))\n",
        "\n",
        "    # Mask invalid keypoints (x < 0 or y < 0)\n",
        "    mask = (keypoints[:, :, 0] >= 0) & (keypoints[:, :, 1] >= 0)  # (B, 21)\n",
        "    mask = mask[:, :, None, None].float()\n",
        "    heatmaps *= mask\n",
        "\n",
        "    # Normalize to have peak of 1\n",
        "    max_vals = heatmaps.amax(dim=(2, 3), keepdim=True) + 1e-5\n",
        "    heatmaps /= max_vals\n",
        "\n",
        "    return heatmaps  # shape: (B, 21, H, W)\n"
      ],
      "metadata": {
        "id": "h0EPGMjFeB0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Test Model method - on each epoch, after training process is finished\n",
        "def test_model(model, test_loader, device, threshold=0.05):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "    correct_preds = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in test_loader:\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            pred_heatmaps = model(images)\n",
        "            preds = soft_argmax(pred_heatmaps)\n",
        "\n",
        "            loss = F.l1_loss(preds, targets, reduction='sum')\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            dists = torch.norm(preds - targets, dim=2)\n",
        "            within_thresh = (dists < threshold).float()\n",
        "\n",
        "            correct_preds += within_thresh.sum().item()\n",
        "            total_samples += dists.numel()\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    avg_time_per_sample = elapsed_time / total_samples\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = correct_preds / total_samples\n",
        "\n",
        "    return avg_loss, accuracy, avg_time_per_sample"
      ],
      "metadata": {
        "id": "-bmCehrFeBxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Section\n",
        "One training method for each model version, one for L1 Loss, one for MSE Loss."
      ],
      "metadata": {
        "id": "hFmmG0lxrzyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coordinates Comparison using L1 Loss"
      ],
      "metadata": {
        "id": "5v5mqCz_tkWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import time\n",
        "\n",
        "chunk_dir = \"/content/drive/MyDrive/HandKeypoints/dataset/\"\n",
        "test_chunk = \"/content/drive/MyDrive/HandKeypoints/dataset/val_chunk0.npz\"\n",
        "chunk_files = sorted([f for f in os.listdir(chunk_dir) if f.endswith(\".npz\") and f.startswith('train')])\n",
        "batch_size = 8\n",
        "\n",
        "final_epoch = 31\n",
        "for epoch in range(start_epoch, final_epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    print(f\"\\nEpoch {epoch}/{final_epoch-1}\")\n",
        "\n",
        "    total_keypoints = 0\n",
        "    # randomize chunk order, then start\n",
        "\n",
        "    step = 0\n",
        "    random.shuffle(chunk_files)\n",
        "    for chunk_file in chunk_files:\n",
        "        step += 1\n",
        "        chunk_path = os.path.join(chunk_dir, chunk_file)\n",
        "\n",
        "        # load one dataset chunk at a time\n",
        "        dataset = HandKeypointsDataset(npz_file=chunk_path)\n",
        "        # DataLoader - wraps iterable around Dataset to enable easy access to the samples\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "        # ProgressBar - show real-time statistics about training\n",
        "        progress_bar = tqdm(dataloader, desc=f\"[{chunk_file}] Step {step}/10\", leave=False, ncols=100, dynamic_ncols=True)\n",
        "\n",
        "        for i, (images, targets) in enumerate(progress_bar):\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            # predict -> apply loss -> update optimizer\n",
        "            pred_heatmaps = model(images)\n",
        "            preds = soft_argmax(pred_heatmaps)\n",
        "\n",
        "            loss = F.l1_loss(preds, targets)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_keypoints += images.size(0) * 21\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    # same as before, load test chunk to obtain test scores\n",
        "    test_dataset = HandKeypointsDataset(test_chunk)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    test_loss, test_acc, test_avg_time = test_model(model, test_loader, device)\n",
        "\n",
        "    # predict first image from last chunk, for visual proof of training\n",
        "    img = images[0].clone().detach().cpu().numpy()\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    img = np.ascontiguousarray(img)\n",
        "    img = (img * 255).clip(0, 255).astype(np.uint8)\n",
        "\n",
        "    predicted = preds[0].detach().cpu()\n",
        "    show_sample(img, predicted, original_size=(224, 224))\n",
        "\n",
        "    # Print epoch statistics\n",
        "    avg_loss = total_loss / total_keypoints\n",
        "    print(f\"Epoch {epoch}, Total Loss: {total_loss:.4f}, Avg Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Avg Time per Keypoint: {test_avg_time:.6f}s\")\n",
        "\n",
        "    history['train_loss'].append(avg_loss)\n",
        "    history['test_loss'].append(test_loss)\n",
        "    history['test_accuracy'].append(test_acc)\n",
        "    history['test_avg_time_per_sample'].append(test_avg_time)\n",
        "\n",
        "    # if better model than before, save it\n",
        "    if test_acc > best_acc:\n",
        "        print(\"Saving model!\")\n",
        "        best_acc = test_acc\n",
        "        torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'total_loss': total_loss,\n",
        "                'avg_loss': avg_loss,\n",
        "                'history': history\n",
        "            }, checkpoint)\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "w4tYNuJsselW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Heatmaps Comparison using MSE Loss"
      ],
      "metadata": {
        "id": "u_MsqR8qtfVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import time\n",
        "\n",
        "chunk_dir = \"/content/drive/MyDrive/HandKeypoints/dataset/\"\n",
        "test_chunk = \"/content/drive/MyDrive/HandKeypoints/dataset/val_chunk0.npz\"\n",
        "chunk_files = sorted([f for f in os.listdir(chunk_dir) if f.endswith(\".npz\") and f.startswith('train')])\n",
        "batch_size = 8\n",
        "\n",
        "final_epoch = 31\n",
        "for epoch in range(start_epoch, final_epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    print(f\"\\nEpoch {epoch}/{final_epoch-1}\")\n",
        "\n",
        "    total_keypoints = 0\n",
        "    # randomize chunk order, then start\n",
        "\n",
        "    step = 0\n",
        "    random.shuffle(chunk_files)\n",
        "    for chunk_file in chunk_files:\n",
        "        step += 1\n",
        "        chunk_path = os.path.join(chunk_dir, chunk_file)\n",
        "\n",
        "        # load one dataset chunk at a time\n",
        "        dataset = HandKeypointsDataset(npz_file=chunk_path)\n",
        "        # DataLoader - wraps iterable around Dataset to enable easy access to the samples\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "        # ProgressBar - show real-time statistics about training\n",
        "        progress_bar = tqdm(dataloader, desc=f\"[{chunk_file}] Step {step}/10\", leave=False, ncols=100, dynamic_ncols=True)\n",
        "\n",
        "        for i, (images, targets) in enumerate(progress_bar):\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            # turn targets to heatmaps\n",
        "            # then predict -> apply loss -> update optimizer\n",
        "            with torch.no_grad():\n",
        "                gt_heatmaps = keypoints_to_heatmaps(targets).to(device)\n",
        "\n",
        "            pred_heatmaps = model(images)\n",
        "\n",
        "            loss = F.mse_loss(pred_heatmaps, gt_heatmaps)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_keypoints += images.size(0) * 21\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    # same as before, load test chunk to obtain test scores\n",
        "    test_dataset = HandKeypointsDataset(test_chunk)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    test_loss, test_acc, test_avg_time = test_model(model, test_loader, device)\n",
        "\n",
        "    # predict first image from last chunk, for visual proof of training\n",
        "    img = images[0].clone().detach().cpu().numpy()\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    img = np.ascontiguousarray(img)\n",
        "    img = (img * 255).clip(0, 255).astype(np.uint8)\n",
        "\n",
        "    # Get coordinates from predicted heatmap and plot results\n",
        "    preds = soft_argmax(pred_heatmaps)\n",
        "    predicted = preds[0].detach().cpu()\n",
        "    show_sample(img, predicted, original_size=(224, 224))\n",
        "\n",
        "    # Print epoch statistics\n",
        "    avg_loss = total_loss / total_keypoints\n",
        "    print(f\"Epoch {epoch}, Total Loss: {total_loss:.4f}, Avg Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Avg Time per Keypoint: {test_avg_time:.6f}s\")\n",
        "\n",
        "    history['train_loss'].append(avg_loss)\n",
        "    history['test_loss'].append(test_loss)\n",
        "    history['test_accuracy'].append(test_acc)\n",
        "    history['test_avg_time_per_sample'].append(test_avg_time)\n",
        "\n",
        "    # if better model than before, save it\n",
        "    if test_acc > best_acc:\n",
        "        print(\"Saving model!\")\n",
        "        best_acc = test_acc\n",
        "        torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'total_loss': total_loss,\n",
        "                'avg_loss': avg_loss,\n",
        "                'history': history\n",
        "            }, checkpoint)\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "LLzBpli1eBvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_path = \"/content/drive/MyDrive/HandKeypoints/dataset/val_chunk0.npz\"\n",
        "dataset = HandKeypointsDataset(test_path)"
      ],
      "metadata": {
        "id": "s9R4g3ZXeiAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_list = [33, 71, 76, 94, 100, 239, 249, 290, 311, 331]\n",
        "for i in img_list:\n",
        "    (image, target) = dataset.__getitem__(i*5)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    test_img = image.unsqueeze(0).to(device)\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        predicted = model(test_img)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Prediction time: {elapsed_time}s\")\n",
        "    predicted_coords = soft_argmax(predicted)\n",
        "    predicted_coords = predicted_coords.squeeze(0).cpu()\n",
        "\n",
        "    predicted = predicted.squeeze(0)\n",
        "    predicted.size()\n",
        "\n",
        "    img = image.squeeze(0).clone().detach().cpu().numpy()\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    img = np.ascontiguousarray(img)\n",
        "    img = (img * 255).clip(0, 255).astype(np.uint8)\n",
        "\n",
        "    print(f\"Image {i}:\")\n",
        "    show_heatmap(img, predicted)\n",
        "    show_sample(img, predicted_coords, original_size=(224, 224))"
      ],
      "metadata": {
        "id": "3kowiDZ7eh9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "export_data = {\n",
        "    'model_name': model_name,\n",
        "    'history': checkpoint.get('history', {}),\n",
        "    'final_epoch': checkpoint.get('epoch'),\n",
        "    'total_loss': checkpoint.get('total_loss'),\n",
        "    'avg_loss': checkpoint.get('avg_loss')\n",
        "}\n",
        "\n",
        "save_dir = '/content/drive/MyDrive/HandKeypoints'\n",
        "save_path = os.path.join(save_dir, f'{model_name}_stats.json')\n",
        "\n",
        "with open(save_path, 'w') as f:\n",
        "    json.dump(export_data, f)"
      ],
      "metadata": {
        "id": "eZDWUvCkehtp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}