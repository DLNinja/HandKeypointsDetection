{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGx81wLMFXKwOh/WppCp0x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DLNinja/HandKeypointsDetection/blob/main/ResNetV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbVtiCxNglJh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "adSZPlwXhPIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display Method"
      ],
      "metadata": {
        "id": "1BFRDA9GlW9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sample(image, coords, original_size=(224, 224), color='bw'):\n",
        "    # Transform to numpy array\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        image = image.cpu().detach().numpy()\n",
        "    if not isinstance(image, np.ndarray):\n",
        "        original_image = np.array(image)\n",
        "    else:\n",
        "        original_image = image\n",
        "\n",
        "    original_image = original_image.copy()\n",
        "    if original_image.dtype != np.uint8:\n",
        "        original_image = (original_image * 255).astype(np.uint8)\n",
        "\n",
        "    # Scale coordinates to image size\n",
        "    img_h, img_w = original_size\n",
        "    if isinstance(coords, torch.Tensor):\n",
        "        coords = coords.detach().cpu().numpy()\n",
        "    rescaled_keypoints = [(int(x * img_w), int(y * img_h)) for x, y in coords]\n",
        "\n",
        "    # Draw the lines - relation of keypoints on hand\n",
        "    line_dict = [\n",
        "        [0, 1, 2, 3, 4],\n",
        "        [0, 5, 6, 7, 8],\n",
        "        [9, 10, 11, 12],\n",
        "        [13, 14, 15, 16],\n",
        "        [0, 17, 18, 19, 20],\n",
        "        [5, 9, 13, 17]\n",
        "    ]\n",
        "    for seq in line_dict:\n",
        "        coords = []\n",
        "        for i in seq:\n",
        "            coords.append(rescaled_keypoints[i])\n",
        "        for i in range(len(coords)-1):\n",
        "            cv2.line(original_image, coords[i], coords[i+1], color=(0, 0, 0), thickness=1)\n",
        "\n",
        "    # Draw keypoints on the original image\n",
        "    for (x, y) in rescaled_keypoints:\n",
        "        cv2.circle(original_image, (x, y), radius=2, color=(255, 255, 255), thickness=-1)\n",
        "\n",
        "    # Display the image\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(original_image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "RaO9oKUZhPFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Loader"
      ],
      "metadata": {
        "id": "ZzBtG1hBlZ5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class HandKeypointsDataset(Dataset):\n",
        "    def __init__(self, npz_file, img_size=(224, 224)):\n",
        "        self.npz_file = npz_file\n",
        "        self.img_size = img_size\n",
        "\n",
        "        self.data = np.load(self.npz_file)\n",
        "        self.images = self.data['images']\n",
        "        self.keypoints = self.data['keypoints']\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data[\"images\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        keypoints = self.keypoints[idx].reshape(21, 3)\n",
        "        keypoints = keypoints[:, :2]        # OutputKeypoints: (21, 2)\n",
        "\n",
        "        image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(keypoints, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "btP8EP4qhPCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition"
      ],
      "metadata": {
        "id": "zlB7gL7xocCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class HeatmapPoseModel(nn.Module):\n",
        "    def __init__(self, num_keypoints=21):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet18(pretrained=True)\n",
        "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])  # Remove classifier\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(256, num_keypoints * 2), # array of x and y for each coordinate\n",
        "            nn.Sigmoid()  # keeps outputs in [0, 1] range\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        return self.fc(x).view(-1, 21, 2)  # return output as 21 pairs of coordinates"
      ],
      "metadata": {
        "id": "Ym0OyTfWhO_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "_t9BJf2zhO5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HeatmapPoseModel().to(device)"
      ],
      "metadata": {
        "id": "fHp2tfQnhO3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing Model Version\n",
        "\n",
        "Two different models were trained:\n",
        "- V2.1: optimizer with LR = 0.0001 (or 1e-4)\n",
        "- V2.2: optimizer with LR = 0.0003 (or 3e-4)"
      ],
      "metadata": {
        "id": "Cgnekz5MiXAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet Model V2.1"
      ],
      "metadata": {
        "id": "AxJU_8AXi2zU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'resnet_v2_1'\n",
        "checkpoint = '/content/drive/MyDrive/HandKeypoints/models/handposeV2_1_checkpoint.pth'\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "cPqDfU8ZhO0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet Model V2.2"
      ],
      "metadata": {
        "id": "tjn6fG1PjV08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'resnet_v2_2'\n",
        "checkpoint = '/content/drive/MyDrive/HandKeypoints/models/handposeV2_2_checkpoint.pth'\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "ZpGjgSGZjWMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load The Model or Start New Instance"
      ],
      "metadata": {
        "id": "VS2mgtYokBWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load pre-trained model from checkpoint\n",
        "if os.path.exists(checkpoint):\n",
        "    checkpoint = torch.load(checkpoint, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    previous_total_loss = checkpoint['total_loss']\n",
        "    previous_avg_loss = checkpoint['avg_loss']\n",
        "    history = checkpoint['history']\n",
        "    best_acc = history['test_accuracy'][-1]\n",
        "    print(f\"Resuming training from epoch {start_epoch}\")\n",
        "else:  # start training new instance\n",
        "    start_epoch = 1\n",
        "    previous_acc = 0\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'test_loss': [],\n",
        "        'test_accuracy': [],\n",
        "        'test_avg_time_per_sample': []\n",
        "    }\n",
        "    best_acc = 0.0"
      ],
      "metadata": {
        "id": "ux2rUpSRhOyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training The Model"
      ],
      "metadata": {
        "id": "4scvzj-RjhAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Model method - on each epoch, after training process is finished\n",
        "def test_model(model, val_paths, device, batch_size=32, threshold=0.05):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "    correct_preds = 0\n",
        "    total_time = 0.0\n",
        "\n",
        "\n",
        "    for val_path in val_paths:\n",
        "      dataset = HandKeypointsDataset(npz_file=val_path)\n",
        "      dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "      start_time = time.time()\n",
        "      with torch.no_grad():\n",
        "          for images, targets in dataloader:\n",
        "              images = images.to(device)\n",
        "              targets = targets.to(device)\n",
        "\n",
        "              preds = model(images)\n",
        "\n",
        "              loss = F.l1_loss(preds, targets, reduction='sum')\n",
        "              total_loss += loss.item()\n",
        "\n",
        "              dists = torch.norm(preds - targets, dim=2)\n",
        "              within_thresh = (dists < threshold).float()\n",
        "\n",
        "              correct_preds += within_thresh.sum().item()\n",
        "              total_samples += dists.numel()\n",
        "      elapsed_time = time.time() - start_time\n",
        "      total_time += elapsed_time\n",
        "    avg_time_per_sample = total_time / total_samples\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = correct_preds / total_samples\n",
        "\n",
        "    return avg_loss, accuracy, avg_time_per_sample"
      ],
      "metadata": {
        "id": "LTJxjH0UhOsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import time\n",
        "\n",
        "chunk_dir = \"/content/drive/MyDrive/HandKeypoints/dataset\"\n",
        "chunk_files = sorted([f for f in os.listdir(chunk_dir) if f.endswith(\".npz\") and f.startswith('train')])\n",
        "val_files = sorted([f for f in os.listdir(chunk_dir) if f.endswith(\".npz\") and f.startswith('val')])\n",
        "batch_size = 32\n",
        "\n",
        "final_epoch = 31\n",
        "for epoch in range(start_epoch, final_epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    print(f\"\\nEpoch {epoch}/{final_epoch-1}\")\n",
        "\n",
        "    total_keypoints = 0\n",
        "    step=0\n",
        "    # randomize chunk order, then start\n",
        "    random.shuffle(chunk_files)\n",
        "    for chunk_file in chunk_files:\n",
        "        step += 1\n",
        "        chunk_path = os.path.join(chunk_dir, chunk_file)\n",
        "\n",
        "        # load one dataset chunk at a time\n",
        "        dataset = HandKeypointsDataset(npz_file=chunk_path)\n",
        "        # DataLoader - wraps iterable around Dataset to enable easy access to the samples\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "        # ProgressBar - show real-time statistics about training\n",
        "        progress_bar = tqdm(dataloader, desc=f\"[{chunk_file}] Step {step}/10\", leave=False, ncols=100, dynamic_ncols=True)\n",
        "\n",
        "        for i, (images, targets) in enumerate(progress_bar):\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            # predict -> apply loss -> update optimizer\n",
        "            preds = model(images)\n",
        "\n",
        "            loss = F.l1_loss(preds, targets, reduction='sum')\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_keypoints += images.size(0) * 21\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    val_paths = [os.path.join(chunk_dir, val_file) for val_file in val_files]\n",
        "    test_loss, test_acc, test_avg_time = test_model(model, val_paths, device, batch_size)\n",
        "\n",
        "    # predict first image from last chunk, for visual proof of training\n",
        "    img = images[0].clone().detach().cpu().numpy()\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    img = np.ascontiguousarray(img)\n",
        "    img = (img * 255).clip(0, 255).astype(np.uint8)\n",
        "\n",
        "    predicted = preds[0].detach().cpu()\n",
        "    show_sample(img, predicted, original_size=(224, 224))\n",
        "\n",
        "    # Print epoch statistics\n",
        "    avg_loss = total_loss / total_keypoints\n",
        "    print(f\"Epoch {epoch}, Total Loss: {total_loss:.4f}, Avg Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Avg Time per Keypoint: {test_avg_time:.6f}s\")\n",
        "\n",
        "    history['train_loss'].append(avg_loss)\n",
        "    history['test_loss'].append(test_loss)\n",
        "    history['test_accuracy'].append(test_acc)\n",
        "    history['test_avg_time_per_sample'].append(test_avg_time)\n",
        "\n",
        "    # if better model than before, save it\n",
        "    if test_acc > best_acc:\n",
        "        print(\"Saving model!\")\n",
        "        best_acc = test_acc\n",
        "        torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'total_loss': total_loss,\n",
        "                'avg_loss': avg_loss,\n",
        "                'history': history\n",
        "            }, checkpoint)"
      ],
      "metadata": {
        "id": "L-5ZAGpfhOpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Section\n",
        "The model is tested on a set of 10 hand-picked images to test the models precision in images of different hand-pose complexity"
      ],
      "metadata": {
        "id": "6Takf2IXkUxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_path = \"/content/drive/MyDrive/HandKeypoints/dataset/val_chunk0.npz\"\n",
        "dataset = HandKeypointsDataset(test_path)"
      ],
      "metadata": {
        "id": "pmVa0tuFhOlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_list = [33, 71, 76, 94, 100, 239, 249, 290, 311, 331]\n",
        "for i in img_list:\n",
        "  (image, target) = dataset.__getitem__(i*5)\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  image = image.unsqueeze(0).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      predicted = model(image)\n",
        "  predicted = predicted.squeeze(0)\n",
        "\n",
        "  img = image.squeeze(0).clone().detach().cpu().numpy()\n",
        "  img = np.transpose(img, (1, 2, 0))\n",
        "  img = np.ascontiguousarray(img)\n",
        "  img = (img * 255).clip(0, 255).astype(np.uint8)\n",
        "\n",
        "  print(f\"Image {i}:\");\n",
        "  show_sample(img, predicted.cpu(), original_size=(224, 224))"
      ],
      "metadata": {
        "id": "40Mv1c36hOiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Training Stats"
      ],
      "metadata": {
        "id": "p6dLM_60k4Cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "export_data = {\n",
        "    'model_name': model_name,\n",
        "    'history': checkpoint.get('history', {}),\n",
        "    'final_epoch': checkpoint.get('epoch'),\n",
        "    'total_loss': checkpoint.get('total_loss'),\n",
        "    'avg_loss': checkpoint.get('avg_loss')\n",
        "}\n",
        "\n",
        "save_dir = '/content/drive/MyDrive/HandKeypoints'\n",
        "save_path = os.path.join(save_dir, f'{model_name}_stats.json')\n",
        "\n",
        "with open(save_path, 'w') as f:\n",
        "    json.dump(export_data, f)"
      ],
      "metadata": {
        "id": "11_-IorBhOgP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}